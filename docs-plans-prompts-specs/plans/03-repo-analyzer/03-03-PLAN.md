---
phase: 03-repo-analyzer
plan: 03
type: execute
domain: python-api
---

<objective>
Finalize Repository Analyzer Agent with caching, error recovery, and pipeline integration.

Purpose: Complete the analyzer with production-ready caching and orchestrator integration.
Output: Cached analysis storage, error recovery, and pipeline stage integration.
</objective>


<context>
@BRIEF.md
@ROADMAP.md
@codestory/backend/agents/repo_analyzer.py
@codestory/backend/skills/repomix/client.py
@codestory/backend/agents/orchestrator.py
</context>

<opus_4_5_considerations>
Production readiness should:
- Cache RepoMix and analysis results to avoid redundant API calls
- Implement graceful error recovery with partial results
- Integrate with pipeline orchestrator as ANALYZE stage
- Track analysis costs (token usage)

Avoid: No caching (expensive re-analysis), silent failures, missing pipeline integration.
</opus_4_5_considerations>

<tasks>

<task type="auto">
  <n>Task 1: Implement analysis caching layer</n>
  <files>codestory/backend/skills/analysis/cache.py</files>
  <action>
Create analysis caching with TTL and invalidation:

```python
"""Analysis caching layer for repository data."""

import hashlib
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any
from dataclasses import dataclass, asdict


@dataclass
class CachedAnalysis:
    """Cached analysis result."""
    repo_url: str
    analysis: dict[str, Any]
    packed_stats: dict[str, Any]
    story_components: list[dict[str, Any]]
    created_at: str
    expires_at: str
    version: str = "1.0"


class AnalysisCache:
    """File-based cache for repository analysis."""
    
    DEFAULT_TTL_HOURS = 24
    
    def __init__(self, cache_dir: Path | None = None):
        """Initialize cache with directory."""
        self.cache_dir = cache_dir or Path.home() / ".codestory" / "cache" / "analysis"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    def _cache_key(self, repo_url: str) -> str:
        """Generate cache key from repo URL."""
        normalized = repo_url.lower().rstrip("/").rstrip(".git")
        return hashlib.sha256(normalized.encode()).hexdigest()[:16]
    
    def _cache_path(self, repo_url: str) -> Path:
        """Get cache file path for repo."""
        return self.cache_dir / f"{self._cache_key(repo_url)}.json"
    
    def get(self, repo_url: str) -> CachedAnalysis | None:
        """Get cached analysis if available and not expired."""
        cache_path = self._cache_path(repo_url)
        
        if not cache_path.exists():
            return None
        
        try:
            data = json.loads(cache_path.read_text())
            expires_at = datetime.fromisoformat(data["expires_at"])
            if datetime.now() > expires_at:
                cache_path.unlink()
                return None
            return CachedAnalysis(**data)
        except (json.JSONDecodeError, KeyError, ValueError):
            cache_path.unlink()
            return None
    
    def set(
        self,
        repo_url: str,
        analysis: dict[str, Any],
        packed_stats: dict[str, Any],
        story_components: list[dict[str, Any]],
        ttl_hours: int | None = None,
    ) -> CachedAnalysis:
        """Cache analysis result with TTL."""
        ttl = ttl_hours or self.DEFAULT_TTL_HOURS
        now = datetime.now()
        
        cached = CachedAnalysis(
            repo_url=repo_url,
            analysis=analysis,
            packed_stats=packed_stats,
            story_components=story_components,
            created_at=now.isoformat(),
            expires_at=(now + timedelta(hours=ttl)).isoformat(),
        )
        
        cache_path = self._cache_path(repo_url)
        cache_path.write_text(json.dumps(asdict(cached), indent=2))
        return cached
    
    def invalidate(self, repo_url: str) -> bool:
        """Invalidate cache for a repository."""
        cache_path = self._cache_path(repo_url)
        if cache_path.exists():
            cache_path.unlink()
            return True
        return False
    
    def clear_all(self) -> int:
        """Clear all cached analyses."""
        count = 0
        for cache_file in self.cache_dir.glob("*.json"):
            cache_file.unlink()
            count += 1
        return count
    
    def stats(self) -> dict[str, Any]:
        """Get cache statistics."""
        files = list(self.cache_dir.glob("*.json"))
        total_size = sum(f.stat().st_size for f in files)
        return {
            "entries": len(files),
            "total_size_kb": total_size / 1024,
            "cache_dir": str(self.cache_dir),
        }


analysis_cache = AnalysisCache()
```

Avoid: In-memory only cache, no expiration, missing invalidation.
  </action>
  <verify>python -c "from codestory.backend.skills.analysis.cache import AnalysisCache; print('Cache OK')"</verify>
  <done>Analysis cache imports successfully</done>
</task>

<task type="auto">
  <n>Task 2: Update Repository Analyzer Agent with caching and pipeline integration</n>
  <files>codestory/backend/agents/repo_analyzer.py</files>
  <action>
Update agent with caching and run_stage method for pipeline:

Add to RepoAnalyzerAgent class:
- Cache integration with use_cache and force_refresh parameters
- run_stage() method for pipeline orchestrator
- cache_stats() method for monitoring
- invalidate_cache() method for manual cache clearing

Key additions:
```python
# In analyze() method:
if use_cache and not force_refresh:
    cached = self.cache.get(repo_url)
    if cached:
        return {
            "repo_url": repo_url,
            "packed_stats": cached.packed_stats,
            "analysis": cached.analysis,
            "story_components": cached.story_components,
            "status": "complete",
            "from_cache": True,
        }

# After successful analysis:
if use_cache:
    self.cache.set(repo_url, analysis_data, packed_stats, story_components)

# Pipeline integration method:
async def run_stage(self, context: dict[str, Any]) -> dict[str, Any]:
    """Run as pipeline ANALYZE stage."""
    return await self.analyze(
        repo_url=context.get("repo_url"),
        intent=context.get("intent"),
    )
```

Avoid: Not checking cache, no pipeline method, missing force_refresh option.
  </action>
  <verify>python -c "from codestory.backend.agents.repo_analyzer import repo_analyzer_agent; print('Agent OK')"</verify>
  <done>Repository Analyzer Agent with caching works</done>
</task>

<task type="auto">
  <n>Task 3: Add cache management API endpoints</n>
  <files>codestory/backend/api/routes/analyze.py</files>
  <action>
Add cache management endpoints to analyze router:

```python
@router.post("/invalidate-cache")
async def invalidate_cache(request: QuickStructureRequest):
    """Invalidate cached analysis for a repository."""
    from backend.agents.repo_analyzer import repo_analyzer_agent
    removed = await repo_analyzer_agent.invalidate_cache(request.repo_url)
    return {"repo_url": request.repo_url, "cache_invalidated": removed}

@router.get("/cache-stats")
async def get_cache_stats():
    """Get analysis cache statistics."""
    from backend.agents.repo_analyzer import repo_analyzer_agent
    return repo_analyzer_agent.cache_stats()

@router.post("/clear-cache")
async def clear_cache():
    """Clear all cached analyses (admin only)."""
    from backend.skills.analysis.cache import analysis_cache
    count = analysis_cache.clear_all()
    return {"cleared": count}
```

Avoid: Missing cache endpoints, no admin cache clear.
  </action>
  <verify>python -c "from codestory.backend.api.routes.analyze import router; print('Router OK')"</verify>
  <done>Cache endpoints configured</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Analysis cache stores and retrieves results
- [ ] Cache has TTL and expiration
- [ ] Repository Analyzer checks cache before analyzing
- [ ] Pipeline run_stage method works
- [ ] Cache management endpoints functional
</verification>

<success_criteria>
- Cached analyses avoid redundant API calls
- Cache has 24-hour default TTL
- force_refresh bypasses cache when needed
- Pipeline ANALYZE stage uses analyzer agent
- Cache stats endpoint reports usage
</success_criteria>

<playwright_validation_gate>
**Playwright MCP Validation Required**

```
# Test cache stats
Playwright Action: HTTP GET /analyze/cache-stats
Expected: 200 OK
Assert: response.json().entries >= 0

# Test analysis (first call - not cached)
Playwright Action: HTTP POST /analyze/full
Body: {"repo_url": "https://github.com/pallets/click"}
Expected: 200 OK
Assert: response.json().from_cache == false

# Test analysis (second call - cached)
Playwright Action: HTTP POST /analyze/full
Body: {"repo_url": "https://github.com/pallets/click"}
Expected: 200 OK
Assert: response.json().from_cache == true

# Test cache invalidation
Playwright Action: HTTP POST /analyze/invalidate-cache
Body: {"repo_url": "https://github.com/pallets/click"}
Expected: 200 OK
Assert: response.json().cache_invalidated == true
```
</playwright_validation_gate>

<o>
After completion, create `plans/03-repo-analyzer/03-03-SUMMARY.md`:

# Phase 3 Plan 3: Repository Analyzer Assembly Summary

**Completed Repository Analyzer with caching and pipeline integration.**

## Accomplishments
- Created file-based analysis cache with TTL
- Updated analyzer agent with cache integration
- Added run_stage() for pipeline orchestrator
- Created cache management API endpoints

## Files Created/Modified
- `codestory/backend/skills/analysis/cache.py` - Analysis cache
- `codestory/backend/agents/repo_analyzer.py` - Cache integration
- `codestory/backend/api/routes/analyze.py` - Cache endpoints

## Decisions Made
- File-based cache (survives restarts)
- 24-hour default TTL
- Cache key from normalized repo URL
- force_refresh option to bypass cache

## Phase 3 Complete
Repository Analyzer Agent fully functional with:
- RepoMix CLI integration for AI-optimized code parsing
- Claude-powered analysis extraction
- Production caching layer
- Pipeline orchestrator integration

## Next Step
Ready for Phase 4: Story Architect Agent
</o>
