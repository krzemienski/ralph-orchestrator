# Plan 03-05: Key Component Identification and Dependency Mapping

## Overview

**Phase**: 3 - Repo Analyzer Agent
**Plan**: 05 of 05
**Depends on**: 03-03 (AST Analysis), 03-04 (Pattern Recognition)
**Enables**: Phase 4 (Story Architect Agent)

## Goal

Create skills that identify the most important components in a codebase and map dependencies between modules. This produces a prioritized list of what to explain in the narrative and visualizes how components interact.

## Tasks

### Task 1: Key Component Identification

Create skill that ranks components by importance based on multiple signals.

**File**: `src/skills/analysis/component_skills.py`

```python
"""Key component identification and importance ranking skills."""

from dataclasses import dataclass, field
from enum import Enum
from typing import Any

from src.skills.base import Skill, skill, handle_skill_errors


class ComponentType(str, Enum):
    """Types of code components."""
    ENTRY_POINT = "entry_point"
    CORE_MODULE = "core_module"
    API_ENDPOINT = "api_endpoint"
    DATA_MODEL = "data_model"
    SERVICE = "service"
    UTILITY = "utility"
    CONFIGURATION = "configuration"
    TEST = "test"


@dataclass
class ComponentInfo:
    """Information about a code component."""
    name: str
    file_path: str
    component_type: ComponentType
    importance_score: float  # 0.0 to 1.0
    reasons: list[str] = field(default_factory=list)
    dependencies: list[str] = field(default_factory=list)
    dependents: list[str] = field(default_factory=list)
    metrics: dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> dict[str, Any]:
        return {
            "name": self.name,
            "file_path": self.file_path,
            "component_type": self.component_type.value,
            "importance_score": self.importance_score,
            "reasons": self.reasons,
            "dependencies": self.dependencies,
            "dependents": self.dependents,
            "metrics": self.metrics,
        }


@dataclass
class DependencyEdge:
    """Represents a dependency relationship."""
    source: str  # Importing module
    target: str  # Imported module
    import_type: str  # "direct", "from", "relative"
    items_imported: list[str] = field(default_factory=list)

    def to_dict(self) -> dict[str, Any]:
        return {
            "source": self.source,
            "target": self.target,
            "import_type": self.import_type,
            "items_imported": self.items_imported,
        }


@dataclass
class DependencyGraph:
    """Complete dependency graph for a codebase."""
    nodes: list[str]  # Module names
    edges: list[DependencyEdge]
    entry_points: list[str]
    leaf_nodes: list[str]  # No outgoing dependencies
    hub_nodes: list[str]  # High connectivity

    def to_dict(self) -> dict[str, Any]:
        return {
            "nodes": self.nodes,
            "edges": [e.to_dict() for e in self.edges],
            "entry_points": self.entry_points,
            "leaf_nodes": self.leaf_nodes,
            "hub_nodes": self.hub_nodes,
            "node_count": len(self.nodes),
            "edge_count": len(self.edges),
        }


# Importance signals and weights
IMPORTANCE_SIGNALS = {
    "is_entry_point": 0.25,
    "high_import_count": 0.20,  # Many other files import this
    "is_api_endpoint": 0.15,
    "is_data_model": 0.15,
    "has_documentation": 0.10,
    "file_size": 0.08,
    "is_in_core_directory": 0.07,
}

# Entry point patterns
ENTRY_POINT_PATTERNS = [
    "main.py", "__main__.py", "app.py", "server.py", "wsgi.py", "asgi.py",
    "manage.py", "cli.py", "run.py", "index.py", "index.ts", "index.js",
    "main.ts", "main.js", "App.tsx", "App.jsx",
]

# Core directory patterns
CORE_DIRECTORY_PATTERNS = [
    "src/", "lib/", "core/", "app/", "api/", "services/", "domain/",
]

# API endpoint patterns
API_PATTERNS = [
    "routes", "routers", "endpoints", "views", "controllers", "api",
]

# Data model patterns
MODEL_PATTERNS = [
    "models", "schemas", "entities", "domain", "types",
]


class ComponentSkills(Skill):
    """Skills for identifying key components and mapping dependencies."""

    @skill(
        name="identify_key_components",
        description="Analyze codebase to identify and rank the most important components based on multiple signals like imports, entry points, and structural position.",
    )
    @handle_skill_errors
    async def identify_key_components(
        self,
        files: list[dict[str, Any]],
        imports_by_file: dict[str, list[str]],
        classes_by_file: dict[str, list[dict]],
        functions_by_file: dict[str, list[dict]],
    ) -> dict[str, Any]:
        """
        Identify and rank key components in the codebase.
        
        Args:
            files: List of file info dicts with 'path', 'size', 'has_docstring'
            imports_by_file: Map of file path to list of imports
            classes_by_file: Map of file path to list of class definitions
            functions_by_file: Map of file path to list of function definitions
        
        Returns:
            Dictionary containing ranked components and analysis summary
        """
        components: list[ComponentInfo] = []
        
        # Calculate import frequency (how often each module is imported)
        import_frequency: dict[str, int] = {}
        for file_path, imports in imports_by_file.items():
            for imp in imports:
                # Normalize import to module name
                module = self._normalize_import(imp)
                import_frequency[module] = import_frequency.get(module, 0) + 1
        
        max_imports = max(import_frequency.values()) if import_frequency else 1
        
        for file_info in files:
            file_path = file_info.get("path", "")
            file_name = file_path.split("/")[-1]
            
            # Skip test files for importance ranking
            if "test" in file_path.lower() and "test" not in file_name.lower():
                continue
            
            score = 0.0
            reasons = []
            component_type = ComponentType.UTILITY
            
            # Check if entry point
            if any(ep in file_name.lower() for ep in ENTRY_POINT_PATTERNS):
                score += IMPORTANCE_SIGNALS["is_entry_point"]
                reasons.append("Entry point file")
                component_type = ComponentType.ENTRY_POINT
            
            # Check import frequency (how many files import this one)
            module_name = self._path_to_module(file_path)
            import_count = import_frequency.get(module_name, 0)
            if import_count > 0:
                normalized_imports = import_count / max_imports
                score += IMPORTANCE_SIGNALS["high_import_count"] * normalized_imports
                if import_count >= 3:
                    reasons.append(f"Imported by {import_count} other modules")
            
            # Check if API endpoint
            if any(pattern in file_path.lower() for pattern in API_PATTERNS):
                score += IMPORTANCE_SIGNALS["is_api_endpoint"]
                reasons.append("API/endpoint definition")
                component_type = ComponentType.API_ENDPOINT
            
            # Check if data model
            if any(pattern in file_path.lower() for pattern in MODEL_PATTERNS):
                score += IMPORTANCE_SIGNALS["is_data_model"]
                reasons.append("Data model definition")
                component_type = ComponentType.DATA_MODEL
            
            # Check documentation
            if file_info.get("has_docstring", False):
                score += IMPORTANCE_SIGNALS["has_documentation"]
                reasons.append("Has documentation")
            
            # File size (larger files often more important, with diminishing returns)
            file_size = file_info.get("size", 0)
            if file_size > 1000:
                size_score = min(file_size / 10000, 1.0) * IMPORTANCE_SIGNALS["file_size"]
                score += size_score
            
            # Core directory bonus
            if any(pattern in file_path.lower() for pattern in CORE_DIRECTORY_PATTERNS):
                score += IMPORTANCE_SIGNALS["is_in_core_directory"]
                reasons.append("Located in core directory")
            
            # Determine component type from classes
            classes = classes_by_file.get(file_path, [])
            functions = functions_by_file.get(file_path, [])
            
            if classes:
                class_names = [c.get("name", "").lower() for c in classes]
                if any("service" in name for name in class_names):
                    component_type = ComponentType.SERVICE
                elif any("model" in name or "entity" in name for name in class_names):
                    component_type = ComponentType.DATA_MODEL
            
            # Get dependencies for this file
            file_imports = imports_by_file.get(file_path, [])
            
            components.append(ComponentInfo(
                name=file_name,
                file_path=file_path,
                component_type=component_type,
                importance_score=min(score, 1.0),
                reasons=reasons,
                dependencies=file_imports[:10],
                metrics={
                    "classes": len(classes),
                    "functions": len(functions),
                    "import_count": import_count,
                    "file_size": file_size,
                },
            ))
        
        # Sort by importance
        components.sort(key=lambda x: x.importance_score, reverse=True)
        
        # Categorize
        entry_points = [c for c in components if c.component_type == ComponentType.ENTRY_POINT]
        core_modules = [c for c in components if c.importance_score >= 0.5]
        
        return {
            "components": [c.to_dict() for c in components[:30]],  # Top 30
            "entry_points": [c.to_dict() for c in entry_points],
            "core_modules": [c.to_dict() for c in core_modules[:10]],
            "total_analyzed": len(components),
            "component_type_distribution": self._get_type_distribution(components),
        }

    @skill(
        name="build_dependency_graph",
        description="Build a dependency graph showing how modules import and depend on each other.",
    )
    @handle_skill_errors
    async def build_dependency_graph(
        self,
        imports_by_file: dict[str, list[dict[str, Any]]],
        internal_modules: list[str],
    ) -> dict[str, Any]:
        """
        Build dependency graph from import information.
        
        Args:
            imports_by_file: Map of file path to list of import dicts with 
                            'module', 'items', 'type' fields
            internal_modules: List of module names that are part of this project
        
        Returns:
            Dictionary containing dependency graph structure
        """
        nodes: set[str] = set()
        edges: list[DependencyEdge] = []
        incoming_count: dict[str, int] = {}
        outgoing_count: dict[str, int] = {}
        
        for file_path, imports in imports_by_file.items():
            source_module = self._path_to_module(file_path)
            nodes.add(source_module)
            
            for imp in imports:
                if isinstance(imp, dict):
                    target_module = imp.get("module", "")
                    import_type = imp.get("type", "direct")
                    items = imp.get("items", [])
                else:
                    target_module = str(imp)
                    import_type = "direct"
                    items = []
                
                # Only include internal dependencies
                if not self._is_internal_module(target_module, internal_modules):
                    continue
                
                nodes.add(target_module)
                edges.append(DependencyEdge(
                    source=source_module,
                    target=target_module,
                    import_type=import_type,
                    items_imported=items[:5],
                ))
                
                # Track connectivity
                outgoing_count[source_module] = outgoing_count.get(source_module, 0) + 1
                incoming_count[target_module] = incoming_count.get(target_module, 0) + 1
        
        # Identify special nodes
        entry_points = [
            n for n in nodes 
            if incoming_count.get(n, 0) == 0 and outgoing_count.get(n, 0) > 0
        ]
        
        leaf_nodes = [
            n for n in nodes 
            if outgoing_count.get(n, 0) == 0 and incoming_count.get(n, 0) > 0
        ]
        
        # Hub nodes: high total connectivity
        hub_threshold = len(edges) / len(nodes) * 2 if nodes else 0
        hub_nodes = [
            n for n in nodes
            if (incoming_count.get(n, 0) + outgoing_count.get(n, 0)) > hub_threshold
        ]
        
        graph = DependencyGraph(
            nodes=list(nodes),
            edges=edges,
            entry_points=entry_points[:5],
            leaf_nodes=leaf_nodes[:10],
            hub_nodes=hub_nodes[:5],
        )
        
        return graph.to_dict()

    @skill(
        name="analyze_module_coupling",
        description="Analyze coupling between modules to identify tightly coupled components and potential refactoring opportunities.",
    )
    @handle_skill_errors
    async def analyze_module_coupling(
        self,
        dependency_graph: dict[str, Any],
    ) -> dict[str, Any]:
        """
        Analyze module coupling from dependency graph.
        
        Args:
            dependency_graph: Dependency graph from build_dependency_graph
        
        Returns:
            Dictionary containing coupling analysis and recommendations
        """
        edges = dependency_graph.get("edges", [])
        nodes = dependency_graph.get("nodes", [])
        
        # Calculate coupling metrics
        module_incoming: dict[str, list[str]] = {}
        module_outgoing: dict[str, list[str]] = {}
        
        for edge in edges:
            source = edge.get("source", "")
            target = edge.get("target", "")
            
            if target not in module_incoming:
                module_incoming[target] = []
            module_incoming[target].append(source)
            
            if source not in module_outgoing:
                module_outgoing[source] = []
            module_outgoing[source].append(target)
        
        # Afferent coupling (Ca): number of modules that depend on this module
        # Efferent coupling (Ce): number of modules this module depends on
        coupling_metrics: list[dict[str, Any]] = []
        
        for node in nodes:
            ca = len(module_incoming.get(node, []))
            ce = len(module_outgoing.get(node, []))
            
            # Instability = Ce / (Ca + Ce), range 0 to 1
            # 0 = stable (many depend on it), 1 = unstable (depends on many)
            instability = ce / (ca + ce) if (ca + ce) > 0 else 0.5
            
            coupling_metrics.append({
                "module": node,
                "afferent_coupling": ca,
                "efferent_coupling": ce,
                "instability": round(instability, 2),
                "dependents": module_incoming.get(node, [])[:5],
                "dependencies": module_outgoing.get(node, [])[:5],
            })
        
        # Sort by total coupling
        coupling_metrics.sort(
            key=lambda x: x["afferent_coupling"] + x["efferent_coupling"],
            reverse=True,
        )
        
        # Identify highly coupled modules
        high_coupling_threshold = 5
        highly_coupled = [
            m for m in coupling_metrics
            if m["afferent_coupling"] + m["efferent_coupling"] >= high_coupling_threshold
        ]
        
        # Calculate average instability
        avg_instability = (
            sum(m["instability"] for m in coupling_metrics) / len(coupling_metrics)
            if coupling_metrics else 0
        )
        
        return {
            "coupling_metrics": coupling_metrics[:20],
            "highly_coupled_modules": highly_coupled[:10],
            "average_instability": round(avg_instability, 2),
            "total_dependencies": len(edges),
            "recommendations": self._generate_coupling_recommendations(
                coupling_metrics, highly_coupled, avg_instability
            ),
        }

    def _normalize_import(self, import_str: str) -> str:
        """Normalize import string to module name."""
        # Handle "from x import y" format
        if " import " in import_str:
            return import_str.split(" import ")[0].replace("from ", "").strip()
        return import_str.replace("import ", "").strip().split(".")[0]

    def _path_to_module(self, file_path: str) -> str:
        """Convert file path to module name."""
        # Remove extension and convert slashes to dots
        module = file_path.replace("/", ".").replace("\\", ".")
        for ext in [".py", ".ts", ".js", ".tsx", ".jsx"]:
            module = module.replace(ext, "")
        # Remove leading src. or similar
        for prefix in ["src.", "lib.", "app."]:
            if module.startswith(prefix):
                module = module[len(prefix):]
        return module

    def _is_internal_module(self, module: str, internal_modules: list[str]) -> bool:
        """Check if module is internal to the project."""
        module_lower = module.lower()
        # Check direct match or prefix match
        for internal in internal_modules:
            if module_lower == internal.lower() or module_lower.startswith(internal.lower() + "."):
                return True
        # Heuristic: if starts with common package names, likely external
        external_prefixes = [
            "os", "sys", "re", "json", "typing", "dataclasses", "enum",
            "collections", "itertools", "functools", "pathlib", "datetime",
            "numpy", "pandas", "requests", "flask", "django", "fastapi",
            "sqlalchemy", "pydantic", "pytest", "unittest",
        ]
        return not any(module_lower.startswith(ext) for ext in external_prefixes)

    def _get_type_distribution(self, components: list[ComponentInfo]) -> dict[str, int]:
        """Get distribution of component types."""
        distribution: dict[str, int] = {}
        for comp in components:
            type_name = comp.component_type.value
            distribution[type_name] = distribution.get(type_name, 0) + 1
        return distribution

    def _generate_coupling_recommendations(
        self,
        metrics: list[dict],
        highly_coupled: list[dict],
        avg_instability: float,
    ) -> list[str]:
        """Generate recommendations based on coupling analysis."""
        recommendations = []
        
        if len(highly_coupled) > 3:
            recommendations.append(
                f"Found {len(highly_coupled)} highly coupled modules. "
                "Consider introducing abstractions or interfaces to reduce coupling."
            )
        
        if avg_instability > 0.7:
            recommendations.append(
                f"Average instability is high ({avg_instability:.2f}). "
                "Many modules depend heavily on others. Consider stabilizing core modules."
            )
        elif avg_instability < 0.3:
            recommendations.append(
                f"Average instability is low ({avg_instability:.2f}). "
                "Architecture appears stable with clear dependency direction."
            )
        
        # Check for potential god modules
        for m in metrics[:5]:
            if m["afferent_coupling"] > 10:
                recommendations.append(
                    f"Module '{m['module']}' has high afferent coupling ({m['afferent_coupling']}). "
                    "Consider if this module has too many responsibilities."
                )
                break
        
        if not recommendations:
            recommendations.append("Coupling metrics appear healthy. No immediate concerns.")
        
        return recommendations
```

### Task 2: Complete Analysis Result Assembly

Create skill that assembles all analysis into a comprehensive result structure for the Story Architect.

**File**: `src/skills/analysis/result_assembler.py`

```python
"""Assembles complete analysis results for Story Architect consumption."""

from dataclasses import dataclass, field
from datetime import datetime
from typing import Any

from src.skills.base import Skill, skill, handle_skill_errors


@dataclass
class AnalysisResult:
    """Complete analysis result for a repository."""
    # Repository metadata
    repo_url: str
    repo_name: str
    analyzed_at: str
    
    # Structure
    file_count: int
    directory_count: int
    primary_language: str
    languages: dict[str, int]
    
    # Architecture
    architectural_patterns: list[dict]
    design_patterns: list[dict]
    framework_patterns: list[dict]
    
    # Components
    entry_points: list[dict]
    key_components: list[dict]
    
    # Dependencies
    dependency_graph: dict
    coupling_analysis: dict
    
    # Code insights
    total_classes: int
    total_functions: int
    average_complexity: float
    
    # For narrative generation
    story_hooks: list[str] = field(default_factory=list)
    suggested_focus_areas: list[str] = field(default_factory=list)
    technical_highlights: list[str] = field(default_factory=list)

    def to_dict(self) -> dict[str, Any]:
        return {
            "metadata": {
                "repo_url": self.repo_url,
                "repo_name": self.repo_name,
                "analyzed_at": self.analyzed_at,
            },
            "structure": {
                "file_count": self.file_count,
                "directory_count": self.directory_count,
                "primary_language": self.primary_language,
                "languages": self.languages,
            },
            "architecture": {
                "architectural_patterns": self.architectural_patterns,
                "design_patterns": self.design_patterns,
                "framework_patterns": self.framework_patterns,
            },
            "components": {
                "entry_points": self.entry_points,
                "key_components": self.key_components,
            },
            "dependencies": {
                "graph": self.dependency_graph,
                "coupling_analysis": self.coupling_analysis,
            },
            "code_metrics": {
                "total_classes": self.total_classes,
                "total_functions": self.total_functions,
                "average_complexity": self.average_complexity,
            },
            "narrative_hints": {
                "story_hooks": self.story_hooks,
                "suggested_focus_areas": self.suggested_focus_areas,
                "technical_highlights": self.technical_highlights,
            },
        }


class ResultAssemblerSkills(Skill):
    """Skills for assembling complete analysis results."""

    @skill(
        name="assemble_analysis_result",
        description="Combine all analysis outputs into a comprehensive result structure optimized for narrative generation.",
    )
    @handle_skill_errors
    async def assemble_analysis_result(
        self,
        repo_url: str,
        repo_name: str,
        structure_analysis: dict[str, Any],
        pattern_analysis: dict[str, Any],
        component_analysis: dict[str, Any],
        dependency_analysis: dict[str, Any],
        ast_analysis: dict[str, Any],
    ) -> dict[str, Any]:
        """
        Assemble all analysis into final result.
        
        Args:
            repo_url: Repository URL
            repo_name: Repository name
            structure_analysis: Output from repository structure analysis
            pattern_analysis: Output from pattern recognition
            component_analysis: Output from key component identification
            dependency_analysis: Output from dependency mapping
            ast_analysis: Output from AST analysis
        
        Returns:
            Complete analysis result dictionary
        """
        # Extract values with defaults
        file_count = structure_analysis.get("file_count", 0)
        directory_count = structure_analysis.get("directory_count", 0)
        languages = structure_analysis.get("languages", {})
        primary_language = max(languages.items(), key=lambda x: x[1])[0] if languages else "unknown"
        
        # Generate story hooks based on analysis
        story_hooks = self._generate_story_hooks(
            pattern_analysis, component_analysis, ast_analysis
        )
        
        # Suggest focus areas based on key components
        suggested_focus = self._suggest_focus_areas(
            component_analysis, pattern_analysis
        )
        
        # Generate technical highlights
        highlights = self._generate_highlights(
            pattern_analysis, dependency_analysis, ast_analysis
        )
        
        result = AnalysisResult(
            repo_url=repo_url,
            repo_name=repo_name,
            analyzed_at=datetime.utcnow().isoformat(),
            file_count=file_count,
            directory_count=directory_count,
            primary_language=primary_language,
            languages=languages,
            architectural_patterns=pattern_analysis.get("architectural_patterns", []),
            design_patterns=pattern_analysis.get("design_patterns", []),
            framework_patterns=pattern_analysis.get("framework_patterns", []),
            entry_points=component_analysis.get("entry_points", []),
            key_components=component_analysis.get("core_modules", []),
            dependency_graph=dependency_analysis.get("graph", {}),
            coupling_analysis=dependency_analysis.get("coupling", {}),
            total_classes=ast_analysis.get("total_classes", 0),
            total_functions=ast_analysis.get("total_functions", 0),
            average_complexity=ast_analysis.get("average_complexity", 0.0),
            story_hooks=story_hooks,
            suggested_focus_areas=suggested_focus,
            technical_highlights=highlights,
        )
        
        return result.to_dict()

    @skill(
        name="generate_analysis_summary",
        description="Generate a human-readable summary of the analysis for quick overview.",
    )
    @handle_skill_errors
    async def generate_analysis_summary(
        self,
        analysis_result: dict[str, Any],
    ) -> dict[str, Any]:
        """
        Generate summary from complete analysis.
        
        Args:
            analysis_result: Complete analysis result dictionary
        
        Returns:
            Summary dictionary with key points
        """
        metadata = analysis_result.get("metadata", {})
        structure = analysis_result.get("structure", {})
        architecture = analysis_result.get("architecture", {})
        components = analysis_result.get("components", {})
        metrics = analysis_result.get("code_metrics", {})
        
        # Build summary sentences
        summary_points = []
        
        # Size summary
        file_count = structure.get("file_count", 0)
        primary_lang = structure.get("primary_language", "unknown")
        summary_points.append(
            f"A {primary_lang} project with {file_count} files"
        )
        
        # Architecture summary
        arch_patterns = architecture.get("architectural_patterns", [])
        if arch_patterns:
            top_pattern = arch_patterns[0].get("pattern_name", "")
            summary_points.append(
                f"Following {top_pattern} architectural pattern"
            )
        
        # Framework summary
        framework_patterns = architecture.get("framework_patterns", [])
        if framework_patterns:
            frameworks = [p.get("pattern_name", "") for p in framework_patterns[:2]]
            summary_points.append(f"Built with {', '.join(frameworks)}")
        
        # Component summary
        entry_points = components.get("entry_points", [])
        key_comps = components.get("key_components", [])
        if entry_points:
            summary_points.append(
                f"{len(entry_points)} entry point(s) and {len(key_comps)} core modules"
            )
        
        # Metrics summary
        classes = metrics.get("total_classes", 0)
        functions = metrics.get("total_functions", 0)
        summary_points.append(f"Contains {classes} classes and {functions} functions")
        
        return {
            "repo_name": metadata.get("repo_name", ""),
            "one_line": summary_points[0] if summary_points else "Repository analyzed",
            "summary_points": summary_points,
            "key_stats": {
                "files": file_count,
                "classes": classes,
                "functions": functions,
                "language": primary_lang,
            },
        }

    def _generate_story_hooks(
        self,
        pattern_analysis: dict,
        component_analysis: dict,
        ast_analysis: dict,
    ) -> list[str]:
        """Generate narrative hooks based on analysis."""
        hooks = []
        
        # Pattern-based hooks
        arch_patterns = pattern_analysis.get("architectural_patterns", [])
        if arch_patterns:
            top = arch_patterns[0]
            hooks.append(
                f"The codebase follows a {top.get('pattern_name', '')} pattern, "
                f"which shapes how data flows through the system"
            )
        
        # Component-based hooks
        entry_points = component_analysis.get("entry_points", [])
        if entry_points:
            entry = entry_points[0]
            hooks.append(
                f"Our journey begins at {entry.get('name', 'the main entry point')}, "
                f"where the application comes to life"
            )
        
        # Complexity hooks
        complexity = ast_analysis.get("average_complexity", 0)
        if complexity > 10:
            hooks.append(
                "Some sophisticated algorithms lurk within, handling complex business logic"
            )
        
        return hooks[:5]

    def _suggest_focus_areas(
        self,
        component_analysis: dict,
        pattern_analysis: dict,
    ) -> list[str]:
        """Suggest areas to focus on in the narrative."""
        focus_areas = []
        
        # Key components
        key_comps = component_analysis.get("core_modules", [])
        for comp in key_comps[:3]:
            focus_areas.append(comp.get("file_path", ""))
        
        # Patterns worth explaining
        design_patterns = pattern_analysis.get("design_patterns", [])
        for pattern in design_patterns[:2]:
            if pattern.get("confidence", 0) > 0.5:
                focus_areas.append(f"Pattern: {pattern.get('pattern_name', '')}")
        
        return focus_areas[:5]

    def _generate_highlights(
        self,
        pattern_analysis: dict,
        dependency_analysis: dict,
        ast_analysis: dict,
    ) -> list[str]:
        """Generate technical highlights."""
        highlights = []
        
        # Architecture highlight
        arch_patterns = pattern_analysis.get("architectural_patterns", [])
        if arch_patterns:
            highlights.append(
                f"Implements {arch_patterns[0].get('pattern_name', '')} architecture"
            )
        
        # Design pattern highlights
        design_patterns = pattern_analysis.get("design_patterns", [])
        if design_patterns:
            patterns = [p.get("pattern_name", "") for p in design_patterns[:3]]
            highlights.append(f"Uses design patterns: {', '.join(patterns)}")
        
        # Dependency insight
        coupling = dependency_analysis.get("coupling", {})
        avg_instability = coupling.get("average_instability", 0.5)
        if avg_instability < 0.4:
            highlights.append("Well-structured with stable core modules")
        
        return highlights[:5]
```

## Files Created/Modified

| File | Action | Purpose |
|------|--------|---------|
| `src/skills/analysis/component_skills.py` | Create | Key component identification and dependency mapping |
| `src/skills/analysis/result_assembler.py` | Create | Assembles complete analysis for Story Architect |
| `src/skills/analysis/__init__.py` | Update | Export new skill classes |
| `src/agents/repo_analyzer.py` | Update | Register ComponentSkills and ResultAssemblerSkills |

## Dependencies

- Plan 03-03 (AST Analysis) provides class/function data
- Plan 03-04 (Pattern Recognition) provides pattern analysis

## Validation Criteria

1. identify_key_components ranks entry points highest
2. build_dependency_graph correctly identifies internal vs external dependencies
3. analyze_module_coupling calculates instability metrics correctly
4. assemble_analysis_result produces complete structure for Story Architect
5. generate_analysis_summary creates readable one-line summary
6. Story hooks are relevant and narratively interesting
7. All imports resolve correctly

## Notes

- Importance scoring uses configurable weights
- Coupling analysis follows established software metrics (Ca, Ce, Instability)
- Result structure designed specifically for Story Architect consumption
- Story hooks provide starting points for narrative generation
