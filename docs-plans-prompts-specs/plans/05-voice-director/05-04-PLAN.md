---
phase: 05-voice-director
plan: 04
type: execute
domain: python-api
---

<objective>
Implement audio assembly that combines synthesized segments into chapter files and complete stories with proper metadata for podcast players and audio apps.
</objective>

<context>
@BRIEF.md
@codestory/backend/skills/voice/synthesis.py
@codestory/backend/skills/voice/preparation.py
</context>

<sub_agent_hint>
Audio assembly involves audio processing expertise distinct from other voice work. Consider sub-agent delegation:

**Complex Audio Processing Tasks:**
- Silence detection and normalization
- Cross-fade transitions between segments
- Volume leveling across chapters
- ID3 metadata and chapter markers
- Multiple output formats (MP3, M4A with chapters, RSS-compatible)

**Benefits of Dedicated Sub-Agent:**
- Audio processing can be tested independently
- FFmpeg/pydub expertise can be isolated
- Output format variations don't block synthesis work
- Quality assurance can iterate on audio parameters

**Sub-Agent Focus Areas:**
1. Segment concatenation with proper transitions
2. Chapter marker insertion for podcast players
3. Metadata embedding (title, author, cover art)
4. Output format conversion and optimization
5. Quality validation (silence, clipping, levels)
</sub_agent_hint>

<tasks>

<task type="auto">
  <n>Task 1: Create audio assembly module</n>
  <files>codestory/backend/skills/voice/assembly.py</files>
  <action>
Create audio assembly with pydub:

```python
"""Audio assembly for Code Story."""

import io
import json
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any
import tempfile

from pydub import AudioSegment

from backend.skills.decorators import skill, handle_skill_errors


@dataclass
class ChapterAudio:
    """Assembled chapter audio."""
    
    chapter_number: int
    title: str
    file_path: str
    duration_ms: int
    start_time_ms: int  # Position in complete story
    segments_included: list[int]


@dataclass
class StoryAudio:
    """Complete assembled story audio."""
    
    title: str
    file_path: str
    total_duration_ms: int
    chapters: list[ChapterAudio]
    metadata: dict[str, Any]


def _create_silence(duration_ms: int) -> AudioSegment:
    """Create silence of specified duration."""
    return AudioSegment.silent(duration=duration_ms)


def _load_audio_file(file_path: str) -> AudioSegment:
    """Load an audio file."""
    return AudioSegment.from_file(file_path)


def _normalize_audio(audio: AudioSegment, target_dbfs: float = -20.0) -> AudioSegment:
    """Normalize audio to target dBFS."""
    change_in_dbfs = target_dbfs - audio.dBFS
    return audio.apply_gain(change_in_dbfs)


def _add_fade(
    audio: AudioSegment,
    fade_in_ms: int = 100,
    fade_out_ms: int = 100,
) -> AudioSegment:
    """Add fade in/out to audio."""
    return audio.fade_in(fade_in_ms).fade_out(fade_out_ms)


@skill(
    name="assemble_chapter",
    description="Assemble audio segments into a chapter",
)
@handle_skill_errors
async def assemble_chapter(
    segment_files: list[dict[str, Any]],
    chapter_number: int,
    chapter_title: str,
    output_path: str,
    include_transitions: bool = True,
    normalize: bool = True,
) -> dict[str, Any]:
    """
    Assemble audio segments into a single chapter file.
    
    Args:
        segment_files: List of segment info with file_path and pause_before
        chapter_number: Chapter number
        chapter_title: Chapter title
        output_path: Path for output file
        include_transitions: Whether to include pause transitions
        normalize: Whether to normalize audio levels
    
    Returns:
        Chapter audio info
    """
    chapter_audio = AudioSegment.empty()
    segments_included = []
    
    for seg in segment_files:
        file_path = seg.get("file_path")
        pause_before = seg.get("pause_before_seconds", 0) * 1000  # Convert to ms
        
        # Add silence before if specified
        if pause_before > 0 and include_transitions:
            chapter_audio += _create_silence(int(pause_before))
        
        # Load and add segment
        segment_audio = _load_audio_file(file_path)
        
        if normalize:
            segment_audio = _normalize_audio(segment_audio)
        
        chapter_audio += segment_audio
        segments_included.append(seg.get("segment_index", 0))
    
    # Add fade in/out
    chapter_audio = _add_fade(chapter_audio)
    
    # Export
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    chapter_audio.export(str(output_file), format="mp3", bitrate="192k")
    
    return {
        "chapter_number": chapter_number,
        "title": chapter_title,
        "file_path": str(output_file),
        "duration_ms": len(chapter_audio),
        "duration_seconds": len(chapter_audio) / 1000,
        "segments_included": segments_included,
        "file_size_bytes": output_file.stat().st_size,
    }


@skill(
    name="assemble_complete_story",
    description="Assemble all chapters into a complete story",
)
@handle_skill_errors
async def assemble_complete_story(
    chapter_files: list[dict[str, Any]],
    story_title: str,
    output_path: str,
    add_chapter_markers: bool = True,
) -> dict[str, Any]:
    """
    Assemble chapter files into a complete story.
    
    Args:
        chapter_files: List of chapter info from assemble_chapter
        story_title: Title of the story
        output_path: Path for complete story file
        add_chapter_markers: Whether to track chapter positions
    
    Returns:
        Complete story audio info with chapter markers
    """
    complete_audio = AudioSegment.empty()
    chapters = []
    current_position = 0
    
    # Add 1 second of silence at the start
    complete_audio += _create_silence(1000)
    current_position += 1000
    
    for chapter in chapter_files:
        chapter_audio = _load_audio_file(chapter["file_path"])
        
        chapters.append({
            "chapter_number": chapter.get("chapter_number", len(chapters) + 1),
            "title": chapter.get("title", f"Chapter {len(chapters) + 1}"),
            "start_time_ms": current_position,
            "start_time_seconds": current_position / 1000,
            "duration_ms": len(chapter_audio),
        })
        
        complete_audio += chapter_audio
        current_position += len(chapter_audio)
        
        # Add 2 seconds between chapters
        complete_audio += _create_silence(2000)
        current_position += 2000
    
    # Final fade out
    complete_audio = complete_audio.fade_out(500)
    
    # Export
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    complete_audio.export(str(output_file), format="mp3", bitrate="192k")
    
    return {
        "title": story_title,
        "file_path": str(output_file),
        "total_duration_ms": len(complete_audio),
        "total_duration_seconds": len(complete_audio) / 1000,
        "total_duration_formatted": _format_duration(len(complete_audio)),
        "chapters": chapters,
        "chapter_count": len(chapters),
        "file_size_bytes": output_file.stat().st_size,
    }


def _format_duration(ms: int) -> str:
    """Format duration as HH:MM:SS."""
    seconds = ms // 1000
    hours = seconds // 3600
    minutes = (seconds % 3600) // 60
    secs = seconds % 60
    
    if hours > 0:
        return f"{hours}:{minutes:02d}:{secs:02d}"
    return f"{minutes}:{secs:02d}"


@skill(
    name="generate_audio_metadata",
    description="Generate metadata for audio files",
)
@handle_skill_errors
async def generate_audio_metadata(
    story_info: dict[str, Any],
    repository_url: str,
    style: str,
    author: str = "Code Story",
) -> dict[str, Any]:
    """
    Generate metadata for audio files (ID3 tags, JSON sidecar).
    
    Args:
        story_info: Story assembly info
        repository_url: Source repository URL
        style: Narrative style used
        author: Author name for metadata
    
    Returns:
        Metadata for embedding and sidecar file
    """
    chapters = story_info.get("chapters", [])
    
    # ID3 tag metadata
    id3_metadata = {
        "title": story_info.get("title", "Code Story"),
        "artist": author,
        "album": "Code Story",
        "genre": "Podcast",
        "year": str(datetime.now().year),
        "comment": f"Generated from {repository_url}",
    }
    
    # Chapter metadata for podcast apps
    chapter_metadata = [
        {
            "title": ch.get("title"),
            "start_time": ch.get("start_time_seconds", 0),
            "end_time": ch.get("start_time_seconds", 0) + ch.get("duration_ms", 0) / 1000,
        }
        for ch in chapters
    ]
    
    # Full JSON metadata
    json_metadata = {
        "version": "1.0",
        "generated_at": datetime.utcnow().isoformat(),
        "source": {
            "repository_url": repository_url,
            "style": style,
        },
        "audio": {
            "title": story_info.get("title"),
            "duration_seconds": story_info.get("total_duration_seconds"),
            "duration_formatted": story_info.get("total_duration_formatted"),
            "file_size_bytes": story_info.get("file_size_bytes"),
            "format": "mp3",
            "bitrate": "192kbps",
        },
        "chapters": chapter_metadata,
        "id3_tags": id3_metadata,
    }
    
    return json_metadata


@skill(
    name="save_metadata_file",
    description="Save metadata to a JSON sidecar file",
)
@handle_skill_errors
async def save_metadata_file(
    metadata: dict[str, Any],
    output_path: str,
) -> dict[str, Any]:
    """
    Save metadata to a JSON file alongside the audio.
    
    Args:
        metadata: Metadata dictionary
        output_path: Path for JSON file
    
    Returns:
        File info
    """
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, "w") as f:
        json.dump(metadata, f, indent=2)
    
    return {
        "file_path": str(output_file),
        "file_size_bytes": output_file.stat().st_size,
    }
```
  </action>
  <verify>python -c "from codestory.backend.skills.voice.assembly import assemble_chapter; print('Assembly OK')"</verify>
  <done>Audio assembly skill created</done>
</task>

<task type="auto">
  <n>Task 2: Add pydub dependency</n>
  <files>pyproject.toml</files>
  <action>
Add pydub to dependencies:

```toml
# In [project.dependencies] section, add:
"pydub>=0.25.1",
```

Also ensure ffmpeg is documented as a system requirement in README.
  </action>
  <verify>uv pip show pydub</verify>
  <done>pydub dependency added</done>
</task>

</tasks>

<playwright_validation_gate>
```
# Full Voice Director pipeline
script = story_architect.create_story(analysis, intent)
prepared = await prepare_script(script["segments"])
synthesis_result = await synthesize_script(prepared["segments"], "documentary", output_dir)
story_audio = await assemble_complete_story(synthesis_result["segments"], script["title"], final_path)
metadata = await generate_audio_metadata(story_audio, github_url, "documentary")

Assert: story_audio["file_path"] exists
Assert: story_audio["total_duration_seconds"] > 60
Assert: len(story_audio["chapters"]) >= 2
Assert: metadata["chapters"] has same length as story_audio["chapters"]
```
</playwright_validation_gate>
