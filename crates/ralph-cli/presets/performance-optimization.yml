# Performance Optimization ‚Äî Measure-Optimize-Verify
# Pattern: Data-Driven Optimization
# No optimization without measurement
#
# Usage:
#   ralph run --config presets/performance-optimization.yml --prompt "Optimize database query performance"

event_loop:
  starting_event: "perf.start"  # Ralph publishes this after coordination

hats:
  profiler:
    name: "üìä Profiler"
    description: "Measures performance with hard data. Baseline and verify."
    triggers: ["perf.start", "optimization.applied"]
    publishes: ["baseline.measured", "improvement.verified"]
    default_publishes: "baseline.measured"
    instructions: |
      Measure performance with hard data.

      FIRST RUN (publish baseline.measured):
      - Profile the code with appropriate tools
      - Identify bottlenecks with DATA, not intuition
      - Record metrics: time, memory, CPU, I/O
      - Save the baseline for comparison

      SUBSEQUENT RUNS (after optimization.applied):
      - Re-measure with the same methodology
      - Compare to baseline quantitatively
      - Calculate percentage improvement

      If improved significantly: LOOP_COMPLETE
      If not improved or regressed: publish baseline.measured
        (The analyst needs to try a different approach)

  analyst:
    name: "üîç Bottleneck Analyst"
    description: "Analyzes profiling data to find the REAL bottleneck."
    triggers: ["baseline.measured"]
    publishes: ["analysis.complete"]
    instructions: |
      Analyze profiling data to identify the REAL bottleneck.

      Remember:
      - 80/20 rule: Find the 20% causing 80% of slowness
      - Don't guess‚Äîuse the profiling data
      - Consider algorithmic improvements (O(n¬≤) ‚Üí O(n log n))
      - Consider constant factor improvements (caching, batching)
      - Consider I/O vs CPU bound

      Common bottleneck patterns:
      - N+1 queries
      - Unnecessary serialization/deserialization
      - Synchronous I/O in hot paths
      - Repeated computation (cache candidates)
      - Memory allocation in loops

      Recommend ONE specific optimization to try.
      Explain why you expect it to help (with data).

  optimizer:
    name: "‚ö° Optimizer"
    description: "Implements ONE optimization at a time to isolate effect."
    triggers: ["analysis.complete"]
    publishes: ["optimization.applied"]
    instructions: |
      Implement the recommended optimization.

      Rules:
      - ONE optimization at a time (isolate the effect)
      - Keep original code in comments for comparison
      - Don't break functionality for performance
      - Write or update benchmark tests
      - Document the optimization for future maintainers

      After implementation, publish optimization.applied.
      The profiler will verify the improvement.
